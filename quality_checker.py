#!/usr/bin/env python3
"""
æå–ç»“æœè´¨é‡è¯„ä¼°å·¥å…·
è‡ªåŠ¨è¯„ä¼°ç”Ÿæˆçš„æ™ºèƒ½ä½“ä»£ç è´¨é‡
"""

import re
import sys
import json
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass


@dataclass
class QualityReport:
    """è´¨é‡æŠ¥å‘Š"""
    score: int  # æ€»åˆ† (0-100)
    grade: str  # ç­‰çº§ (A/B/C/D/F)
    metrics: Dict[str, any]  # å„é¡¹æŒ‡æ ‡
    issues: List[str]  # é—®é¢˜åˆ—è¡¨
    suggestions: List[str]  # æ”¹è¿›å»ºè®®


class QualityChecker:
    """è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.required_sections = [
            'defun',
            'ç›®æ ‡',
            'æ€ç»´æ¨¡å‹',
        ]
        
        self.recommended_sections = [
            'äººæ ¼ç‰¹è´¨',
            'æ ¸å¿ƒä¿¡å¿µ',
            'è¯­è¨€æ­¦å™¨åº“',
            'æ‰§è¡Œæµç¨‹',
            'è´¨é‡æ£€éªŒæ ‡å‡†',
            'ç¦å¿Œæ¸…å•',
        ]
    
    def check_file(self, file_path: str) -> QualityReport:
        """æ£€æŸ¥æ–‡ä»¶è´¨é‡"""
        try:
            content = self._read_file(file_path)
            
            # æ‰§è¡Œå„é¡¹æ£€æŸ¥
            structure_score, structure_issues = self._check_structure(content)
            syntax_score, syntax_issues = self._check_syntax(content)
            content_score, content_issues = self._check_content_quality(content)
            completeness_score, completeness_issues = self._check_completeness(content)
            
            # è®¡ç®—æ€»åˆ†
            total_score = int(
                structure_score * 0.25 +
                syntax_score * 0.25 +
                content_score * 0.30 +
                completeness_score * 0.20
            )
            
            # ç¡®å®šç­‰çº§
            grade = self._calculate_grade(total_score)
            
            # æ”¶é›†æ‰€æœ‰é—®é¢˜
            all_issues = (
                structure_issues + 
                syntax_issues + 
                content_issues + 
                completeness_issues
            )
            
            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            suggestions = self._generate_suggestions(
                total_score, 
                structure_score,
                syntax_score,
                content_score,
                completeness_score
            )
            
            # æ”¶é›†æŒ‡æ ‡
            metrics = {
                'ç»“æ„å®Œæ•´æ€§': f"{structure_score}/100",
                'Lispè¯­æ³•': f"{syntax_score}/100",
                'å†…å®¹è´¨é‡': f"{content_score}/100",
                'å®Œæ•´åº¦': f"{completeness_score}/100",
                'æ–‡ä»¶å¤§å°': f"{len(content)} å­—ç¬¦",
                'æ€»è¡Œæ•°': len(content.split('\n')),
            }
            
            return QualityReport(
                score=total_score,
                grade=grade,
                metrics=metrics,
                issues=all_issues,
                suggestions=suggestions
            )
            
        except Exception as e:
            return QualityReport(
                score=0,
                grade='F',
                metrics={},
                issues=[f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"],
                suggestions=["æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®", "ç¡®ä¿æ–‡ä»¶æ ¼å¼æ­£ç¡®"]
            )
    
    def _read_file(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶"""
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _check_structure(self, content: str) -> Tuple[int, List[str]]:
        """æ£€æŸ¥ç»“æ„å®Œæ•´æ€§"""
        score = 100
        issues = []
        
        # æ£€æŸ¥å¿…éœ€éƒ¨åˆ†
        for section in self.required_sections:
            if section not in content:
                score -= 30
                issues.append(f"âŒ ç¼ºå°‘å¿…éœ€éƒ¨åˆ†: {section}")
        
        # æ£€æŸ¥æ¨èéƒ¨åˆ†
        missing_recommended = 0
        for section in self.recommended_sections:
            if section not in content:
                missing_recommended += 1
        
        # æ¯ç¼ºå°‘ä¸€ä¸ªæ¨èéƒ¨åˆ†æ‰£5åˆ†
        score -= missing_recommended * 5
        
        if missing_recommended > 0:
            issues.append(f"âš ï¸  ç¼ºå°‘ {missing_recommended} ä¸ªæ¨èéƒ¨åˆ†")
        
        return max(0, score), issues
    
    def _check_syntax(self, content: str) -> Tuple[int, List[str]]:
        """æ£€æŸ¥ Lisp è¯­æ³•"""
        score = 100
        issues = []
        
        # æ£€æŸ¥æ‹¬å·é…å¯¹
        open_count = content.count('(')
        close_count = content.count(')')
        
        if open_count != close_count:
            diff = abs(open_count - close_count)
            score -= min(50, diff * 5)
            issues.append(f"âŒ æ‹¬å·ä¸é…å¯¹: å·¦æ‹¬å· {open_count} ä¸ª, å³æ‹¬å· {close_count} ä¸ª")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„ defun ç»“æ„
        defun_pattern = r'\(defun\s+\S+\s+\(\)'
        if not re.search(defun_pattern, content):
            score -= 20
            issues.append("âš ï¸  defun å‡½æ•°å®šä¹‰æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æ³¨é‡Š
        if not re.search(r';.*[\u4e00-\u9fa5]', content):
            score -= 10
            issues.append("âš ï¸  ç¼ºå°‘ä¸­æ–‡æ³¨é‡Š")
        
        return max(0, score), issues
    
    def _check_content_quality(self, content: str) -> Tuple[int, List[str]]:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        score = 100
        issues = []
        
        # æ£€æŸ¥é•¿åº¦
        if len(content) < 500:
            score -= 40
            issues.append("âŒ å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æå–ä¸å®Œæ•´")
        elif len(content) < 1000:
            score -= 20
            issues.append("âš ï¸  å†…å®¹è¾ƒçŸ­ï¼Œå»ºè®®ä¸°å¯Œç»†èŠ‚")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“å†…å®¹ï¼ˆä¸æ˜¯ç©ºæ³›çš„å ä½ç¬¦ï¼‰
        if '...' in content or 'å¾…è¡¥å……' in content or 'TODO' in content:
            score -= 15
            issues.append("âš ï¸  åŒ…å«å ä½ç¬¦æˆ–å¾…å®Œæˆå†…å®¹")
        
        # æ£€æŸ¥æ€ç»´æ¨¡å‹æ•°é‡
        thinking_models = re.findall(r'\([^)]+æ³•|\([^)]+æ€ç»´|\([^)]+æ¨¡å‹', content)
        if len(thinking_models) < 2:
            score -= 20
            issues.append("âš ï¸  æ€ç»´æ¨¡å‹æ•°é‡åå°‘ï¼ˆå»ºè®®3-5ä¸ªï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“æ­¥éª¤
        steps = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ­¥', content)
        if len(steps) < 3:
            score -= 15
            issues.append("âš ï¸  æ‰§è¡Œæ­¥éª¤è¾ƒå°‘ï¼ˆå»ºè®®3-7æ­¥ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®ä¾‹æˆ–ç¤ºä¾‹
        if 'ç¤ºä¾‹' not in content and 'ä¾‹å¦‚' not in content and 'æ¡ˆä¾‹' not in content:
            score -= 10
            issues.append("ğŸ’¡ å»ºè®®æ·»åŠ ç¤ºä¾‹æˆ–æ¡ˆä¾‹")
        
        return max(0, score), issues
    
    def _check_completeness(self, content: str) -> Tuple[int, List[str]]:
        """æ£€æŸ¥å®Œæ•´åº¦"""
        score = 100
        issues = []
        
        # æ£€æŸ¥å„éƒ¨åˆ†çš„å†…å®¹æ˜¯å¦å……å®
        sections_to_check = {
            'ç›®æ ‡': 20,
            'æ ¸å¿ƒä¿¡å¿µ': 30,
            'æ€ç»´æ¨¡å‹': 50,
        }
        
        for section, expected_min_length in sections_to_check.items():
            if section in content:
                # æå–è¯¥éƒ¨åˆ†çš„å†…å®¹
                pattern = f'{section}[^)]*\\)'
                matches = re.findall(pattern, content)
                if matches:
                    section_content = matches[0]
                    if len(section_content) < expected_min_length:
                        score -= 15
                        issues.append(f"âš ï¸  '{section}' éƒ¨åˆ†å†…å®¹è¿‡äºç®€å•")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä½¿ç”¨æŒ‡å—
        if 'ä½¿ç”¨æŒ‡å—' not in content and 'start' not in content:
            score -= 10
            issues.append("ğŸ’¡ å»ºè®®æ·»åŠ ä½¿ç”¨æŒ‡å—æˆ–å¯åŠ¨å‡½æ•°")
        
        return max(0, score), issues
    
    def _calculate_grade(self, score: int) -> str:
        """è®¡ç®—ç­‰çº§"""
        if score >= 90:
            return 'A'
        elif score >= 80:
            return 'B'
        elif score >= 70:
            return 'C'
        elif score >= 60:
            return 'D'
        else:
            return 'F'
    
    def _generate_suggestions(
        self, 
        total_score: int,
        structure_score: int,
        syntax_score: int,
        content_score: int,
        completeness_score: int
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        if total_score >= 90:
            suggestions.append("âœ¨ è´¨é‡ä¼˜ç§€ï¼å¯ä»¥ç›´æ¥ä½¿ç”¨")
        elif total_score >= 80:
            suggestions.append("ğŸ‘ è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¼˜åŒ–åˆ°90åˆ†ä»¥ä¸Š")
        else:
            suggestions.append("âš ï¸  è´¨é‡éœ€è¦æ”¹è¿›")
        
        if structure_score < 80:
            suggestions.append("ğŸ“ è¡¥å……ç¼ºå¤±çš„ç»“æ„éƒ¨åˆ†ï¼ˆäººæ ¼ç‰¹è´¨ã€æ ¸å¿ƒä¿¡å¿µç­‰ï¼‰")
        
        if syntax_score < 80:
            suggestions.append("ğŸ”§ ä¿®å¤ Lisp è¯­æ³•é—®é¢˜ï¼Œç¡®ä¿æ‹¬å·é…å¯¹")
        
        if content_score < 80:
            suggestions.append("ğŸ“š ä¸°å¯Œå†…å®¹ç»†èŠ‚ï¼Œæ·»åŠ æ›´å¤šæ€ç»´æ¨¡å‹å’Œæ­¥éª¤")
            suggestions.append("ğŸ’¡ æ·»åŠ å…·ä½“ç¤ºä¾‹å’Œæ¡ˆä¾‹")
        
        if completeness_score < 80:
            suggestions.append("âœï¸  æ‰©å……å„éƒ¨åˆ†å†…å®¹ï¼Œé¿å…è¿‡äºç®€å•")
        
        if total_score < 70:
            suggestions.append("ğŸ”„ å»ºè®®ä¼˜åŒ–è¾“å…¥æ–‡æ¡ˆåé‡æ–°æå–")
        
        return suggestions
    
    def print_report(self, file_path: str, report: QualityReport):
        """æ‰“å°æŠ¥å‘Š"""
        print("=" * 60)
        print(f"ğŸ“Š è´¨é‡è¯„ä¼°æŠ¥å‘Š: {Path(file_path).name}")
        print("=" * 60)
        print()
        
        # æ€»åˆ†å’Œç­‰çº§
        print(f"ğŸ¯ æ€»åˆ†: {report.score}/100  ç­‰çº§: {report.grade}")
        print()
        
        # è¯¦ç»†æŒ‡æ ‡
        print("ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
        print("-" * 60)
        for metric, value in report.metrics.items():
            print(f"  {metric:.<30} {value}")
        print()
        
        # é—®é¢˜åˆ—è¡¨
        if report.issues:
            print("âš ï¸  å‘ç°çš„é—®é¢˜:")
            print("-" * 60)
            for issue in report.issues:
                print(f"  {issue}")
            print()
        else:
            print("âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
            print()
        
        # æ”¹è¿›å»ºè®®
        if report.suggestions:
            print("ğŸ’¡ æ”¹è¿›å»ºè®®:")
            print("-" * 60)
            for i, suggestion in enumerate(report.suggestions, 1):
                print(f"  {i}. {suggestion}")
            print()
        
        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æå–ç»“æœè´¨é‡è¯„ä¼°å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ£€æŸ¥å•ä¸ªæ–‡ä»¶
  python quality_checker.py output/extracted_xxx.lisp
  
  # æ£€æŸ¥å¹¶è¾“å‡º JSON æ ¼å¼
  python quality_checker.py output/extracted_xxx.lisp --json
  
  # æ‰¹é‡æ£€æŸ¥ output ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
  python quality_checker.py output/*.lisp
        """
    )
    
    parser.add_argument(
        'files',
        nargs='+',
        help='è¦æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šä¸ªæ–‡ä»¶ï¼‰'
    )
    
    parser.add_argument(
        '--json',
        action='store_true',
        help='ä»¥ JSON æ ¼å¼è¾“å‡ºç»“æœ'
    )
    
    parser.add_argument(
        '--min-score',
        type=int,
        default=0,
        help='æœ€ä½åˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤åˆ†æ•°çš„æ–‡ä»¶ä¼šæ ‡è®°ï¼ˆé»˜è®¤: 0ï¼‰'
    )
    
    args = parser.parse_args()
    
    checker = QualityChecker()
    results = []
    
    for file_path in args.files:
        report = checker.check_file(file_path)
        
        if args.json:
            results.append({
                'file': file_path,
                'score': report.score,
                'grade': report.grade,
                'metrics': report.metrics,
                'issues': report.issues,
                'suggestions': report.suggestions,
            })
        else:
            checker.print_report(file_path, report)
            
            if report.score < args.min_score:
                print(f"âš ï¸  è­¦å‘Š: åˆ†æ•° {report.score} ä½äºé˜ˆå€¼ {args.min_score}")
                print()
    
    # JSON è¾“å‡º
    if args.json:
        print(json.dumps(results, ensure_ascii=False, indent=2))
    
    # å¦‚æœæ£€æŸ¥äº†å¤šä¸ªæ–‡ä»¶ï¼Œæ˜¾ç¤ºæ±‡æ€»
    if len(args.files) > 1 and not args.json:
        print("=" * 60)
        print("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
        print("=" * 60)
        
        all_reports = [checker.check_file(f) for f in args.files]
        avg_score = sum(r.score for r in all_reports) / len(all_reports)
        
        print(f"æ–‡ä»¶æ€»æ•°: {len(all_reports)}")
        print(f"å¹³å‡åˆ†æ•°: {avg_score:.1f}/100")
        print(f"æœ€é«˜åˆ†æ•°: {max(r.score for r in all_reports)}/100")
        print(f"æœ€ä½åˆ†æ•°: {min(r.score for r in all_reports)}/100")
        print()
        
        # ç­‰çº§åˆ†å¸ƒ
        grade_dist = {}
        for report in all_reports:
            grade_dist[report.grade] = grade_dist.get(report.grade, 0) + 1
        
        print("ç­‰çº§åˆ†å¸ƒ:")
        for grade in ['A', 'B', 'C', 'D', 'F']:
            if grade in grade_dist:
                print(f"  {grade}: {'â–ˆ' * grade_dist[grade]} ({grade_dist[grade]})")
        print()


if __name__ == '__main__':
    main()

